{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Part1_LinearRegression_pseudo.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykJcVKYVAizK"
      },
      "source": [
        "# Simple Linear versus Ridge Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsSoBj6EAizR"
      },
      "source": [
        "## Step 1:  Getting, understanding, and preprocessing the dataset\n",
        "\n",
        "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAuAPbdFAizS"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.linear_model\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chjht3ARAizT"
      },
      "source": [
        "###  Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYScrtZzAizT"
      },
      "source": [
        "# Import the boston dataset from sklearn\n",
        "# Load dataset to some variable \n",
        "boston_data = load_boston()\n",
        "# print(boston_data.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJQHCANlAizT",
        "outputId": "456531ac-b662-48f0-f37b-251b750d14f9"
      },
      "source": [
        "#  Create X and Y variables - X holding the .data and Y holding .target \n",
        "X = boston_data.data\n",
        "y = boston_data.target\n",
        "\n",
        "#  Reshape Y to be a rank 2 matrix using y.reshape()\n",
        "Y = y.reshape(X.shape[0], 1)\n",
        "# print(Y)\n",
        "\n",
        "# Observe the number of features and the number of labels\n",
        "print('The number of features is: ', X.shape[1])\n",
        "# Printing out the features\n",
        "print('The features: ', boston_data.feature_names)\n",
        "# The number of examples\n",
        "print('The number of examples in our dataset: ', X.shape[0])\n",
        "# Observing the first 2 rows of the data\n",
        "print(X[0:2])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of features is:  13\n",
            "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
            " 'B' 'LSTAT']\n",
            "The number of examples in our dataset:  506\n",
            "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
            "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
            "  4.9800e+00]\n",
            " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
            "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
            "  9.1400e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TQ92iyoAizU"
      },
      "source": [
        "We will also create polynomial feeatures for the dataset to test linear and ridge regression on data with d = 1 and data with d = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgkLFIjeAizU"
      },
      "source": [
        "# Create a PolynomialFeatures object with degree = 2. Using PolynomialFeatures(degree=2)\n",
        "# Transform X and save it into X_2 using poly.fit_transform(X)\n",
        "# Simply copy Y into Y_2 \n",
        "\n",
        "pfeatures = PolynomialFeatures(degree=2)\n",
        "  \n",
        "# transforms the existing features to higher degree features.\n",
        "X_2 = pfeatures.fit_transform(X)\n",
        "y_2 = Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOjA80OgAizU",
        "outputId": "40876ec8-5477-40a9-ff53-5263a10f4433"
      },
      "source": [
        "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
        "print(X_2.shape)\n",
        "print(y_2.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506, 105)\n",
            "(506, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dScrmha0AizU"
      },
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Vfa2FcAizV"
      },
      "source": [
        "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
        "# Return w values\n",
        "\n",
        "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
        "    I = np.identity(X_train.shape[1])\n",
        "    w = np.dot(np.dot(np.linalg.inv(np.dot(X_train.T, X_train) + alpha * I), X_train.T), y_train)\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffH-yaCOAizV"
      },
      "source": [
        "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
        "# Return w values\n",
        "\n",
        "def get_coeff_linear_normaleq(X_train, y_train):\n",
        "    w = np.dot(np.dot(np.linalg.pinv(np.dot(X_train.T, X_train)), X_train.T), y_train)\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTus3UUjAizV"
      },
      "source": [
        "# Define the evaluate_err_ridge function.\n",
        "# Return the train_error and test_error values\n",
        "\n",
        "def evaluate_err(X_train, X_test, y_train, y_test, w): \n",
        "    pred_train = np.dot(X_train, w)\n",
        "    pred_test = np.dot(X_test, w)\n",
        "    train_error = np.mean(np.square(y_train - pred_train))\n",
        "    test_error = np.mean(np.square(y_test - pred_test))\n",
        "    \n",
        "    return train_error, test_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01iYAdxZAizV"
      },
      "source": [
        "# Finish writting the k_fold_cross_validation function. \n",
        "# Returns the average training error and average test error from the k-fold cross validation\n",
        "# Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "\n",
        "def k_fold_cross_validation(k, X, y, alpha=None):\n",
        "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
        "    total_E_val_test = 0\n",
        "    total_E_val_train = 0\n",
        "    train_ = []\n",
        "    test_ = []\n",
        "    print(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        \n",
        "        # Centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
        "        # Subtract y_train_mean from y_train and y_test\n",
        "        y_train_mean = np.mean(y_train)\n",
        "        y_train -= y_train_mean\n",
        "        y_test -= y_train_mean\n",
        "        \n",
        "        # Scaling the data matrix\n",
        "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "        # And scaler.transform(...)\n",
        "        X_train = scaler.transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "        \n",
        "        # Determine the training error and the test error\n",
        "        # Use get_coeff_linear_normaleq or get_coeff_ridge_normaleq to get w\n",
        "        if alpha is None:\n",
        "          m = \"Linear\"\n",
        "          w = get_coeff_linear_normaleq(X_train, y_train)\n",
        "        else:\n",
        "          m = \"Ridge\"\n",
        "          w = get_coeff_ridge_normaleq(X_train, y_train, alpha)\n",
        "        # And use evaluate_err()\n",
        "        total_E_val_train = evaluate_err(X_train, X_test, y_train, y_test, w)[0]\n",
        "        total_E_val_test = evaluate_err(X_train, X_test, y_train, y_test, w)[1]\n",
        "        train_.append(total_E_val_train)\n",
        "        test_.append(total_E_val_test)\n",
        "        print(\"<<Training Error>> {:.3f}\".format(total_E_val_train), \" <<Testing Error>> {:.3f}\".format(total_E_val_test))\n",
        "    train_average = np.mean(train_)\n",
        "    test_average = np.mean(test_)\n",
        "    print(\"\\n<<Average Training Error>> {:.3f}\".format(train_average), \" <<Average Testing Error>> {:.3f}\".format(test_average))\n",
        "    print(\"*-*-*-*-*-*-*-*-*-*-* {} *-*-*-*-*-*-*-*-*-*-*\\n\".format(m))\n",
        "       ##############\n",
        "    return  total_E_val_test, total_E_val_train\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dSzALokAizW"
      },
      "source": [
        "# print the error for the both linear regression and ridge regression\n",
        "# the error should include both training error and testing error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKtPAyOOAizW"
      },
      "source": [
        "# testÂ the various polynomial regressions (requirement 6) asked in the question, and the various regularization lambdas (requirement 4)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GdJFrr9uDem",
        "outputId": "6b6ab402-d90e-47ae-ad2f-a0d4cc562423"
      },
      "source": [
        "# LINEAR\n",
        "test, train = k_fold_cross_validation(10, X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 20.244  <<Testing Error>> 37.104\n",
            "<<Training Error>> 22.585  <<Testing Error>> 17.124\n",
            "<<Training Error>> 21.210  <<Testing Error>> 29.085\n",
            "<<Training Error>> 22.104  <<Testing Error>> 20.751\n",
            "<<Training Error>> 22.726  <<Testing Error>> 15.481\n",
            "<<Training Error>> 22.086  <<Testing Error>> 21.169\n",
            "<<Training Error>> 23.218  <<Testing Error>> 10.543\n",
            "<<Training Error>> 21.192  <<Testing Error>> 29.642\n",
            "<<Training Error>> 22.361  <<Testing Error>> 18.710\n",
            "<<Training Error>> 20.335  <<Testing Error>> 36.751\n",
            "\n",
            "<<Average Training Error>> 21.806  <<Average Testing Error>> 23.636\n",
            "*-*-*-*-*-*-*-*-*-*-* Linear *-*-*-*-*-*-*-*-*-*-*\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l-2qLyy8VF2",
        "outputId": "fb83fa14-3e17-4e2f-ac0a-1cd99bc110af"
      },
      "source": [
        "# RIDGE with different lambda values\n",
        "for i in np.logspace(1, 7, num = 13):\n",
        "  print(\"lambda: \", i)\n",
        "  k_fold_cross_validation(10, X, y, i)\n",
        "\n",
        "##########################################\n",
        "# Best model performace when lambda = 10 with the average training error of 21.893 and the average testing error of 23.689."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lambda:  10.0\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 20.332  <<Testing Error>> 37.032\n",
            "<<Training Error>> 22.671  <<Testing Error>> 17.227\n",
            "<<Training Error>> 21.307  <<Testing Error>> 28.983\n",
            "<<Training Error>> 22.188  <<Testing Error>> 21.102\n",
            "<<Training Error>> 22.810  <<Testing Error>> 15.281\n",
            "<<Training Error>> 22.168  <<Testing Error>> 21.004\n",
            "<<Training Error>> 23.311  <<Testing Error>> 10.152\n",
            "<<Training Error>> 21.283  <<Testing Error>> 30.047\n",
            "<<Training Error>> 22.441  <<Testing Error>> 18.600\n",
            "<<Training Error>> 20.417  <<Testing Error>> 37.457\n",
            "\n",
            "<<Average Training Error>> 21.893  <<Average Testing Error>> 23.689\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  31.622776601683793\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 20.733  <<Testing Error>> 37.184\n",
            "<<Training Error>> 23.066  <<Testing Error>> 17.637\n",
            "<<Training Error>> 21.725  <<Testing Error>> 29.418\n",
            "<<Training Error>> 22.567  <<Testing Error>> 21.850\n",
            "<<Training Error>> 23.199  <<Testing Error>> 14.946\n",
            "<<Training Error>> 22.552  <<Testing Error>> 20.676\n",
            "<<Training Error>> 23.728  <<Testing Error>> 9.850\n",
            "<<Training Error>> 21.683  <<Testing Error>> 31.397\n",
            "<<Training Error>> 22.817  <<Testing Error>> 18.401\n",
            "<<Training Error>> 20.786  <<Testing Error>> 38.819\n",
            "\n",
            "<<Average Training Error>> 22.285  <<Average Testing Error>> 24.018\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  100.0\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 22.191  <<Testing Error>> 38.289\n",
            "<<Training Error>> 24.532  <<Testing Error>> 19.410\n",
            "<<Training Error>> 23.177  <<Testing Error>> 31.533\n",
            "<<Training Error>> 23.947  <<Testing Error>> 23.650\n",
            "<<Training Error>> 24.672  <<Testing Error>> 14.101\n",
            "<<Training Error>> 24.025  <<Testing Error>> 20.000\n",
            "<<Training Error>> 25.239  <<Testing Error>> 9.826\n",
            "<<Training Error>> 23.078  <<Testing Error>> 35.851\n",
            "<<Training Error>> 24.257  <<Testing Error>> 18.138\n",
            "<<Training Error>> 22.138  <<Testing Error>> 42.142\n",
            "\n",
            "<<Average Training Error>> 23.725  <<Average Testing Error>> 25.294\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  316.22776601683796\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 26.642  <<Testing Error>> 42.211\n",
            "<<Training Error>> 28.972  <<Testing Error>> 25.608\n",
            "<<Training Error>> 27.511  <<Testing Error>> 37.667\n",
            "<<Training Error>> 28.279  <<Testing Error>> 27.907\n",
            "<<Training Error>> 29.391  <<Testing Error>> 13.516\n",
            "<<Training Error>> 28.682  <<Testing Error>> 20.498\n",
            "<<Training Error>> 29.889  <<Testing Error>> 11.317\n",
            "<<Training Error>> 27.163  <<Testing Error>> 47.101\n",
            "<<Training Error>> 28.799  <<Testing Error>> 18.907\n",
            "<<Training Error>> 26.337  <<Testing Error>> 49.841\n",
            "\n",
            "<<Average Training Error>> 28.167  <<Average Testing Error>> 29.457\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  1000.0\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 37.064  <<Testing Error>> 50.893\n",
            "<<Training Error>> 39.073  <<Testing Error>> 39.614\n",
            "<<Training Error>> 37.540  <<Testing Error>> 50.003\n",
            "<<Training Error>> 38.578  <<Testing Error>> 38.125\n",
            "<<Training Error>> 40.507  <<Testing Error>> 17.718\n",
            "<<Training Error>> 39.533  <<Testing Error>> 26.252\n",
            "<<Training Error>> 40.744  <<Testing Error>> 17.147\n",
            "<<Training Error>> 36.443  <<Testing Error>> 66.347\n",
            "<<Training Error>> 39.596  <<Testing Error>> 23.347\n",
            "<<Training Error>> 36.244  <<Testing Error>> 65.449\n",
            "\n",
            "<<Average Training Error>> 38.532  <<Average Testing Error>> 39.489\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  3162.2776601683795\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 52.787  <<Testing Error>> 63.541\n",
            "<<Training Error>> 53.924  <<Testing Error>> 59.702\n",
            "<<Training Error>> 52.673  <<Testing Error>> 66.293\n",
            "<<Training Error>> 54.016  <<Testing Error>> 53.961\n",
            "<<Training Error>> 56.711  <<Testing Error>> 29.375\n",
            "<<Training Error>> 55.579  <<Testing Error>> 38.339\n",
            "<<Training Error>> 56.795  <<Testing Error>> 28.222\n",
            "<<Training Error>> 50.694  <<Testing Error>> 87.099\n",
            "<<Training Error>> 55.855  <<Testing Error>> 32.207\n",
            "<<Training Error>> 51.037  <<Testing Error>> 87.733\n",
            "\n",
            "<<Average Training Error>> 54.007  <<Average Testing Error>> 54.647\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  10000.0\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 68.340  <<Testing Error>> 76.760\n",
            "<<Training Error>> 68.469  <<Testing Error>> 79.207\n",
            "<<Training Error>> 67.834  <<Testing Error>> 81.594\n",
            "<<Training Error>> 69.270  <<Testing Error>> 69.258\n",
            "<<Training Error>> 72.236  <<Testing Error>> 43.253\n",
            "<<Training Error>> 71.182  <<Testing Error>> 51.783\n",
            "<<Training Error>> 72.436  <<Testing Error>> 40.479\n",
            "<<Training Error>> 65.461  <<Testing Error>> 103.909\n",
            "<<Training Error>> 71.896  <<Testing Error>> 42.370\n",
            "<<Training Error>> 65.475  <<Testing Error>> 108.572\n",
            "\n",
            "<<Average Training Error>> 69.260  <<Average Testing Error>> 69.719\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  31622.776601683792\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 77.773  <<Testing Error>> 85.186\n",
            "<<Training Error>> 77.295  <<Testing Error>> 90.960\n",
            "<<Training Error>> 77.130  <<Testing Error>> 90.851\n",
            "<<Training Error>> 78.562  <<Testing Error>> 78.377\n",
            "<<Training Error>> 81.542  <<Testing Error>> 52.109\n",
            "<<Training Error>> 80.588  <<Testing Error>> 60.240\n",
            "<<Training Error>> 81.901  <<Testing Error>> 48.238\n",
            "<<Training Error>> 74.671  <<Testing Error>> 113.575\n",
            "<<Training Error>> 81.650  <<Testing Error>> 49.010\n",
            "<<Training Error>> 74.194  <<Testing Error>> 120.670\n",
            "\n",
            "<<Average Training Error>> 78.531  <<Average Testing Error>> 78.922\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  100000.0\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 81.706  <<Testing Error>> 88.775\n",
            "<<Training Error>> 80.981  <<Testing Error>> 95.849\n",
            "<<Training Error>> 81.027  <<Testing Error>> 94.725\n",
            "<<Training Error>> 82.448  <<Testing Error>> 82.156\n",
            "<<Training Error>> 85.410  <<Testing Error>> 55.853\n",
            "<<Training Error>> 84.504  <<Testing Error>> 63.805\n",
            "<<Training Error>> 85.851  <<Testing Error>> 51.521\n",
            "<<Training Error>> 78.553  <<Testing Error>> 117.567\n",
            "<<Training Error>> 85.726  <<Testing Error>> 51.861\n",
            "<<Training Error>> 77.827  <<Testing Error>> 125.614\n",
            "\n",
            "<<Average Training Error>> 82.403  <<Average Testing Error>> 82.772\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  316227.7660168379\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 83.078  <<Testing Error>> 90.036\n",
            "<<Training Error>> 82.267  <<Testing Error>> 97.553\n",
            "<<Training Error>> 82.389  <<Testing Error>> 96.078\n",
            "<<Training Error>> 83.805  <<Testing Error>> 83.471\n",
            "<<Training Error>> 86.758  <<Testing Error>> 57.164\n",
            "<<Training Error>> 85.870  <<Testing Error>> 65.052\n",
            "<<Training Error>> 87.229  <<Testing Error>> 52.671\n",
            "<<Training Error>> 79.912  <<Testing Error>> 118.957\n",
            "<<Training Error>> 87.149  <<Testing Error>> 52.866\n",
            "<<Training Error>> 79.094  <<Testing Error>> 127.326\n",
            "\n",
            "<<Average Training Error>> 83.755  <<Average Testing Error>> 84.117\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  1000000.0\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 83.526  <<Testing Error>> 90.449\n",
            "<<Training Error>> 82.688  <<Testing Error>> 98.109\n",
            "<<Training Error>> 82.834  <<Testing Error>> 96.521\n",
            "<<Training Error>> 84.248  <<Testing Error>> 83.901\n",
            "<<Training Error>> 87.199  <<Testing Error>> 57.592\n",
            "<<Training Error>> 86.316  <<Testing Error>> 65.460\n",
            "<<Training Error>> 87.680  <<Testing Error>> 53.047\n",
            "<<Training Error>> 80.357  <<Testing Error>> 119.411\n",
            "<<Training Error>> 87.614  <<Testing Error>> 53.195\n",
            "<<Training Error>> 79.508  <<Testing Error>> 127.884\n",
            "\n",
            "<<Average Training Error>> 84.197  <<Average Testing Error>> 84.557\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  3162277.6601683795\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 83.669  <<Testing Error>> 90.581\n",
            "<<Training Error>> 82.822  <<Testing Error>> 98.287\n",
            "<<Training Error>> 82.976  <<Testing Error>> 96.662\n",
            "<<Training Error>> 84.390  <<Testing Error>> 84.038\n",
            "<<Training Error>> 87.339  <<Testing Error>> 57.729\n",
            "<<Training Error>> 86.458  <<Testing Error>> 65.590\n",
            "<<Training Error>> 87.824  <<Testing Error>> 53.168\n",
            "<<Training Error>> 80.499  <<Testing Error>> 119.556\n",
            "<<Training Error>> 87.763  <<Testing Error>> 53.300\n",
            "<<Training Error>> 79.640  <<Testing Error>> 128.062\n",
            "\n",
            "<<Average Training Error>> 84.338  <<Average Testing Error>> 84.697\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "lambda:  10000000.0\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "<<Training Error>> 83.715  <<Testing Error>> 90.623\n",
            "<<Training Error>> 82.864  <<Testing Error>> 98.343\n",
            "<<Training Error>> 83.021  <<Testing Error>> 96.707\n",
            "<<Training Error>> 84.435  <<Testing Error>> 84.082\n",
            "<<Training Error>> 87.384  <<Testing Error>> 57.773\n",
            "<<Training Error>> 86.503  <<Testing Error>> 65.632\n",
            "<<Training Error>> 87.869  <<Testing Error>> 53.206\n",
            "<<Training Error>> 80.544  <<Testing Error>> 119.602\n",
            "<<Training Error>> 87.810  <<Testing Error>> 53.334\n",
            "<<Training Error>> 79.682  <<Testing Error>> 128.119\n",
            "\n",
            "<<Average Training Error>> 84.383  <<Average Testing Error>> 84.742\n",
            "*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tqsdsPXt80Y",
        "outputId": "e2551163-9032-412f-bbd7-c37cfdc62d80"
      },
      "source": [
        "# LINEAR (degree = 2)\n",
        "test_2, train_2 = k_fold_cross_validation(10, X_2, y_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<<Training Error>> 4.893  <<Testing Error>> 19.750\n",
            "<<Training Error>> 6.022  <<Testing Error>> 18.438\n",
            "<<Training Error>> 6.067  <<Testing Error>> 8.606\n",
            "<<Training Error>> 6.154  <<Testing Error>> 7.242\n",
            "<<Training Error>> 6.035  <<Testing Error>> 8.004\n",
            "<<Training Error>> 5.960  <<Testing Error>> 9.025\n",
            "<<Training Error>> 5.969  <<Testing Error>> 7.999\n",
            "<<Training Error>> 5.882  <<Testing Error>> 10.427\n",
            "<<Training Error>> 5.594  <<Testing Error>> 13.269\n",
            "<<Training Error>> 5.513  <<Testing Error>> 15.790\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Linear *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 5.809  <<Average Testing Error>> 11.855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClofLjP_AizW",
        "outputId": "9ba8ba89-38d0-45b0-cd38-73a0eef622d5"
      },
      "source": [
        "# RIDGE (degree = 2) with different lambda values\n",
        "for i in np.logspace(1, 7, num = 13):\n",
        "  k_fold_cross_validation(10, X_2, y_2, i)\n",
        "  print(\"lambda: \", i)\n",
        "\n",
        "##########################################\n",
        "# Best model performace when lambda = 10 with the average training error of 10.049 and the average testing error of 13.476."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<<Training Error>> 9.184  <<Testing Error>> 21.366\n",
            "<<Training Error>> 10.419  <<Testing Error>> 9.051\n",
            "<<Training Error>> 10.066  <<Testing Error>> 14.244\n",
            "<<Training Error>> 10.573  <<Testing Error>> 6.474\n",
            "<<Training Error>> 10.001  <<Testing Error>> 16.944\n",
            "<<Training Error>> 10.230  <<Testing Error>> 11.701\n",
            "<<Training Error>> 10.424  <<Testing Error>> 7.712\n",
            "<<Training Error>> 10.165  <<Testing Error>> 12.238\n",
            "<<Training Error>> 10.149  <<Testing Error>> 10.188\n",
            "<<Training Error>> 9.279  <<Testing Error>> 24.843\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 10.049  <<Average Testing Error>> 13.476\n",
            "lambda:  10.0\n",
            "<<Training Error>> 11.827  <<Testing Error>> 24.896\n",
            "<<Training Error>> 13.135  <<Testing Error>> 10.934\n",
            "<<Training Error>> 12.595  <<Testing Error>> 17.894\n",
            "<<Training Error>> 13.391  <<Testing Error>> 8.274\n",
            "<<Training Error>> 12.670  <<Testing Error>> 17.654\n",
            "<<Training Error>> 12.852  <<Testing Error>> 14.925\n",
            "<<Training Error>> 13.276  <<Testing Error>> 8.351\n",
            "<<Training Error>> 12.845  <<Testing Error>> 16.189\n",
            "<<Training Error>> 13.051  <<Testing Error>> 11.454\n",
            "<<Training Error>> 11.876  <<Testing Error>> 27.726\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 12.752  <<Average Testing Error>> 15.830\n",
            "lambda:  31.622776601683793\n",
            "<<Training Error>> 15.153  <<Testing Error>> 29.516\n",
            "<<Training Error>> 16.692  <<Testing Error>> 13.346\n",
            "<<Training Error>> 15.973  <<Testing Error>> 21.859\n",
            "<<Training Error>> 17.004  <<Testing Error>> 11.441\n",
            "<<Training Error>> 16.270  <<Testing Error>> 17.750\n",
            "<<Training Error>> 16.244  <<Testing Error>> 17.784\n",
            "<<Training Error>> 17.000  <<Testing Error>> 9.300\n",
            "<<Training Error>> 16.186  <<Testing Error>> 22.091\n",
            "<<Training Error>> 16.643  <<Testing Error>> 14.219\n",
            "<<Training Error>> 15.062  <<Testing Error>> 32.493\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 16.223  <<Average Testing Error>> 18.980\n",
            "lambda:  100.0\n",
            "<<Training Error>> 18.465  <<Testing Error>> 34.159\n",
            "<<Training Error>> 20.310  <<Testing Error>> 16.327\n",
            "<<Training Error>> 19.405  <<Testing Error>> 25.992\n",
            "<<Training Error>> 20.484  <<Testing Error>> 15.594\n",
            "<<Training Error>> 19.987  <<Testing Error>> 16.798\n",
            "<<Training Error>> 19.782  <<Testing Error>> 19.423\n",
            "<<Training Error>> 20.742  <<Testing Error>> 9.988\n",
            "<<Training Error>> 19.444  <<Testing Error>> 28.930\n",
            "<<Training Error>> 20.167  <<Testing Error>> 16.188\n",
            "<<Training Error>> 18.216  <<Testing Error>> 37.288\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 19.700  <<Average Testing Error>> 22.069\n",
            "lambda:  316.22776601683796\n",
            "<<Training Error>> 22.883  <<Testing Error>> 39.743\n",
            "<<Training Error>> 24.970  <<Testing Error>> 21.788\n",
            "<<Training Error>> 23.886  <<Testing Error>> 31.905\n",
            "<<Training Error>> 24.955  <<Testing Error>> 20.907\n",
            "<<Training Error>> 24.931  <<Testing Error>> 15.467\n",
            "<<Training Error>> 24.501  <<Testing Error>> 21.068\n",
            "<<Training Error>> 25.609  <<Testing Error>> 10.958\n",
            "<<Training Error>> 23.704  <<Testing Error>> 39.089\n",
            "<<Training Error>> 24.834  <<Testing Error>> 17.331\n",
            "<<Training Error>> 22.601  <<Testing Error>> 43.927\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 24.287  <<Average Testing Error>> 26.218\n",
            "lambda:  1000.0\n",
            "<<Training Error>> 31.484  <<Testing Error>> 48.423\n",
            "<<Training Error>> 33.613  <<Testing Error>> 33.183\n",
            "<<Training Error>> 32.462  <<Testing Error>> 42.735\n",
            "<<Training Error>> 33.683  <<Testing Error>> 30.378\n",
            "<<Training Error>> 34.411  <<Testing Error>> 16.780\n",
            "<<Training Error>> 33.598  <<Testing Error>> 25.266\n",
            "<<Training Error>> 34.864  <<Testing Error>> 14.549\n",
            "<<Training Error>> 31.740  <<Testing Error>> 56.694\n",
            "<<Training Error>> 33.920  <<Testing Error>> 20.553\n",
            "<<Training Error>> 31.092  <<Testing Error>> 57.242\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 33.087  <<Average Testing Error>> 34.580\n",
            "lambda:  3162.2776601683795\n",
            "<<Training Error>> 45.154  <<Testing Error>> 59.606\n",
            "<<Training Error>> 46.857  <<Testing Error>> 50.067\n",
            "<<Training Error>> 45.728  <<Testing Error>> 58.352\n",
            "<<Training Error>> 47.243  <<Testing Error>> 45.240\n",
            "<<Training Error>> 49.037  <<Testing Error>> 24.395\n",
            "<<Training Error>> 47.866  <<Testing Error>> 33.917\n",
            "<<Training Error>> 49.224  <<Testing Error>> 22.630\n",
            "<<Training Error>> 44.066  <<Testing Error>> 78.218\n",
            "<<Training Error>> 48.251  <<Testing Error>> 28.001\n",
            "<<Training Error>> 44.193  <<Testing Error>> 77.468\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 46.762  <<Average Testing Error>> 47.790\n",
            "lambda:  10000.0\n",
            "<<Training Error>> 60.738  <<Testing Error>> 71.508\n",
            "<<Training Error>> 61.501  <<Testing Error>> 69.079\n",
            "<<Training Error>> 60.639  <<Testing Error>> 74.369\n",
            "<<Training Error>> 62.282  <<Testing Error>> 61.512\n",
            "<<Training Error>> 64.862  <<Testing Error>> 36.904\n",
            "<<Training Error>> 63.699  <<Testing Error>> 45.809\n",
            "<<Training Error>> 65.049  <<Testing Error>> 34.038\n",
            "<<Training Error>> 58.348  <<Testing Error>> 96.467\n",
            "<<Training Error>> 64.287  <<Testing Error>> 37.659\n",
            "<<Training Error>> 58.684  <<Testing Error>> 99.117\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 62.009  <<Average Testing Error>> 62.646\n",
            "lambda:  31622.776601683792\n",
            "<<Training Error>> 73.359  <<Testing Error>> 81.750\n",
            "<<Training Error>> 73.233  <<Testing Error>> 84.995\n",
            "<<Training Error>> 72.857  <<Testing Error>> 86.693\n",
            "<<Training Error>> 74.424  <<Testing Error>> 74.051\n",
            "<<Training Error>> 77.288  <<Testing Error>> 48.214\n",
            "<<Training Error>> 76.271  <<Testing Error>> 56.493\n",
            "<<Training Error>> 77.615  <<Testing Error>> 44.359\n",
            "<<Training Error>> 70.398  <<Testing Error>> 109.320\n",
            "<<Training Error>> 77.194  <<Testing Error>> 46.088\n",
            "<<Training Error>> 70.239  <<Testing Error>> 115.417\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 74.288  <<Average Testing Error>> 74.738\n",
            "lambda:  100000.0\n",
            "<<Training Error>> 79.933  <<Testing Error>> 87.340\n",
            "<<Training Error>> 79.344  <<Testing Error>> 93.444\n",
            "<<Training Error>> 79.295  <<Testing Error>> 93.051\n",
            "<<Training Error>> 80.773  <<Testing Error>> 80.437\n",
            "<<Training Error>> 83.703  <<Testing Error>> 54.265\n",
            "<<Training Error>> 82.772  <<Testing Error>> 62.267\n",
            "<<Training Error>> 84.127  <<Testing Error>> 49.946\n",
            "<<Training Error>> 76.816  <<Testing Error>> 115.852\n",
            "<<Training Error>> 83.928  <<Testing Error>> 50.658\n",
            "<<Training Error>> 76.235  <<Testing Error>> 123.540\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 80.693  <<Average Testing Error>> 81.080\n",
            "lambda:  316227.7660168379\n",
            "<<Training Error>> 82.469  <<Testing Error>> 89.537\n",
            "<<Training Error>> 81.705  <<Testing Error>> 96.726\n",
            "<<Training Error>> 81.792  <<Testing Error>> 95.503\n",
            "<<Training Error>> 83.228  <<Testing Error>> 82.884\n",
            "<<Training Error>> 86.172  <<Testing Error>> 56.617\n",
            "<<Training Error>> 85.275  <<Testing Error>> 64.521\n",
            "<<Training Error>> 86.637  <<Testing Error>> 52.129\n",
            "<<Training Error>> 79.314  <<Testing Error>> 118.367\n",
            "<<Training Error>> 86.531  <<Testing Error>> 52.449\n",
            "<<Training Error>> 78.547  <<Testing Error>> 126.618\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 83.167  <<Average Testing Error>> 83.535\n",
            "lambda:  1000000.0\n",
            "<<Training Error>> 83.329  <<Testing Error>> 90.286\n",
            "<<Training Error>> 82.505  <<Testing Error>> 97.841\n",
            "<<Training Error>> 82.640  <<Testing Error>> 96.334\n",
            "<<Training Error>> 84.061  <<Testing Error>> 83.710\n",
            "<<Training Error>> 87.008  <<Testing Error>> 57.415\n",
            "<<Training Error>> 86.123  <<Testing Error>> 65.287\n",
            "<<Training Error>> 87.487  <<Testing Error>> 52.871\n",
            "<<Training Error>> 80.162  <<Testing Error>> 119.220\n",
            "<<Training Error>> 87.413  <<Testing Error>> 53.059\n",
            "<<Training Error>> 79.330  <<Testing Error>> 127.655\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 84.006  <<Average Testing Error>> 84.368\n",
            "lambda:  3162277.6601683795\n",
            "<<Training Error>> 83.606  <<Testing Error>> 90.529\n",
            "<<Training Error>> 82.764  <<Testing Error>> 98.201\n",
            "<<Training Error>> 82.914  <<Testing Error>> 96.603\n",
            "<<Training Error>> 84.330  <<Testing Error>> 83.977\n",
            "<<Training Error>> 87.279  <<Testing Error>> 57.673\n",
            "<<Training Error>> 86.397  <<Testing Error>> 65.535\n",
            "<<Training Error>> 87.762  <<Testing Error>> 53.111\n",
            "<<Training Error>> 80.437  <<Testing Error>> 119.495\n",
            "<<Training Error>> 87.699  <<Testing Error>> 53.257\n",
            "<<Training Error>> 79.583  <<Testing Error>> 127.989\n",
            "\n",
            "*-*-*-*-*-*-*-*-*-*-*-* Ridge *-*-*-*-*-*-*-*-*-*-*-*\n",
            "\n",
            "<<Average Training Error>> 84.277  <<Average Testing Error>> 84.637\n",
            "lambda:  10000000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_hcBGS49VrH"
      },
      "source": [
        "##########################################\n",
        "################# REPORT #################\n",
        "##########################################\n",
        "\n",
        "# Based on the resulted average errors from each ridge and linear regression, \n",
        "# linear regression has the best model performance with the lowest training/testing errors when degree = 2\n",
        "# with lambda = 10, the average training error of 5.809, and the average testing error of 11.855.\n",
        "\n",
        "# If I were given a choice of predicting future housing prices using one of the models, \n",
        "# I would choose to use a linear regression with polynomial transformation of degree 2\n",
        "# because it produces both the lowest training and testing errors as presented."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}